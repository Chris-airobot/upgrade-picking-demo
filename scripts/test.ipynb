{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Depth Camera test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "import rospy\n",
    "from ggcnn.srv import GraspPrediction\n",
    "from sensor_msgs.msg import Image\n",
    "import cv_bridge\n",
    "bridge = cv_bridge.CvBridge()\n",
    "import cv2\n",
    "import helpers.transforms as tfh\n",
    "from geometry_msgs.msg import PoseStamped, PoseArray\n",
    "from std_msgs.msg import Header\n",
    "from helpers.timeit import TimeIt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_depth_image(depth, crop_size=500, out_size=300, return_mask=False, crop_y_offset=20):\n",
    "    imh, imw = depth.shape\n",
    "\n",
    "    with TimeIt('1'):\n",
    "        # Crop.\n",
    "        depth_crop = depth[(imh - crop_size) // 2 - crop_y_offset:(imh - crop_size) // 2 + crop_size - crop_y_offset,\n",
    "                           (imw - crop_size) // 2:(imw - crop_size) // 2 + crop_size]\n",
    "    # depth_nan_mask = np.isnan(depth_crop).astype(np.uint8)\n",
    "\n",
    "    # Inpaint\n",
    "    # OpenCV inpainting does weird things at the border.\n",
    "    with TimeIt('2'):\n",
    "        depth_crop = cv2.copyMakeBorder(depth_crop, 1, 1, 1, 1, cv2.BORDER_DEFAULT)\n",
    "        depth_nan_mask = np.isnan(depth_crop).astype(np.uint8)\n",
    "\n",
    "    with TimeIt('3'):\n",
    "        depth_crop[depth_nan_mask==1] = 0\n",
    "\n",
    "    with TimeIt('4'):\n",
    "        # Scale to keep as float, but has to be in bounds -1:1 to keep opencv happy.\n",
    "        depth_scale = np.abs(depth_crop).max()\n",
    "        depth_crop = depth_crop.astype(np.float32) / depth_scale  # Has to be float32, 64 not supported.\n",
    "\n",
    "        with TimeIt('Inpainting'):\n",
    "            depth_crop = cv2.inpaint(depth_crop, depth_nan_mask, 1, cv2.INPAINT_NS)\n",
    "\n",
    "        # Back to original size and value range.\n",
    "        depth_crop = depth_crop[1:-1, 1:-1]\n",
    "        depth_crop = depth_crop * depth_scale\n",
    "\n",
    "    with TimeIt('5'):\n",
    "        # Resize\n",
    "        depth_crop = cv2.resize(depth_crop, (out_size, out_size), cv2.INTER_AREA)\n",
    "\n",
    "    if return_mask:\n",
    "        with TimeIt('6'):\n",
    "            depth_nan_mask = depth_nan_mask[1:-1, 1:-1]\n",
    "            depth_nan_mask = cv2.resize(depth_nan_mask, (out_size, out_size), cv2.INTER_NEAREST)\n",
    "        return depth_crop, depth_nan_mask\n",
    "    else:\n",
    "        return depth_crop\n",
    "\n",
    "def depth_img_callback(msg):\n",
    "    # Doing a rospy.wait_for_message is super slow, compared to just subscribing and keeping the newest one.\n",
    "    curr_depth_img = bridge.imgmsg_to_cv2(msg)\n",
    "    depth_crop, depth_nan_mask = process_depth_image(curr_depth_img,return_mask=True)\n",
    "    cv2.imshow('Processed Depth Image', depth_crop)\n",
    "    cv2.waitKey(1)\n",
    "    \n",
    "rospy.init_node(\"testjaja\")\n",
    "\n",
    "# Subscribe to the depth image topic\n",
    "rospy.Subscriber(\"/camera/depth/image_rect_raw\", Image, depth_img_callback)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual service tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "import rospy\n",
    "from ggcnn.srv import GraspPrediction\n",
    "from sensor_msgs.msg import Image\n",
    "import cv_bridge\n",
    "bridge = cv_bridge.CvBridge()\n",
    "import cv2\n",
    "import helpers.transforms as tfh\n",
    "from geometry_msgs.msg import PoseStamped, PoseArray\n",
    "from std_msgs.msg import Header\n",
    "from helpers.timeit import TimeIt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "rospy.init_node(\"testjaja\")\n",
    "pose_pub = rospy.Publisher(\"/pose_viz\", PoseArray, queue_size=1)\n",
    "ggcnn_service_name = '/ggcnn_service'\n",
    "rospy.wait_for_service(ggcnn_service_name + '/predict')\n",
    "ggcnn_srv = rospy.ServiceProxy(ggcnn_service_name + '/predict', GraspPrediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your pose is: position: \n",
      "  x: 31.492901863825296\n",
      "  y: 134.06626385162474\n",
      "  z: -604.1765036814106\n",
      "orientation: \n",
      "  x: 0.9970603796069337\n",
      "  y: 0.07661983697501132\n",
      "  z: 4.691611905131982e-18\n",
      "  w: 6.105234012211382e-17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# while True:\n",
    "ret = ggcnn_srv.call()\n",
    "\n",
    "\n",
    "best_grasp = ret.best_grasp\n",
    "\n",
    "\n",
    "tfh.publish_pose_as_transform(best_grasp.pose, 'base_link', 'G', 0.5)\n",
    "print(f'Your pose is: {best_grasp.pose}')\n",
    "\n",
    "\n",
    "grasp_pose_stamped = PoseStamped(\n",
    "    pose=best_grasp.pose, header=Header(frame_id=\"base_link\")\n",
    ")\n",
    "pose_pub.publish(\n",
    "    PoseArray(header=Header(frame_id=\"base_link\"), poses=[best_grasp.pose])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 300, 300, 1)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 100, 100, 32)         2624      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 50, 50, 16)           12816     ['conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 25, 25, 8)            1160      ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2D  (None, 50, 50, 8)            584       ['conv2d_3[0][0]']            \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2D  (None, 100, 100, 16)         3216      ['conv2d_transpose_1[0][0]']  \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2D  (None, 300, 300, 32)         41504     ['conv2d_transpose_2[0][0]']  \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " pos_out (Conv2D)            (None, 300, 300, 1)          129       ['conv2d_transpose_3[0][0]']  \n",
      "                                                                                                  \n",
      " cos_out (Conv2D)            (None, 300, 300, 1)          129       ['conv2d_transpose_3[0][0]']  \n",
      "                                                                                                  \n",
      " sin_out (Conv2D)            (None, 300, 300, 1)          129       ['conv2d_transpose_3[0][0]']  \n",
      "                                                                                                  \n",
      " width_out (Conv2D)          (None, 300, 300, 1)          129       ['conv2d_transpose_3[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 62420 (243.83 KB)\n",
      "Trainable params: 62420 (243.83 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Path to your HDF5 file\n",
    "model_path = '/home/riot/kinova_gen3_lite/src/ggcnn/scripts/models/epoch_29_model.hdf5'\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Print the model summary to verify it loaded correctly\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ggcnn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
